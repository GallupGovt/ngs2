---
title: |
  | Group Innovation under Competition and Uncertaity
author: "Pablo Diego-Rosell, PhD  - Gallup"
date: "November 20, 2018"
output:
  html_document:
    number_sections: yes
    theme: united
    toc: yes
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---

```{r setup, include=FALSE}
rm(list = ls(all = TRUE))
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, strip.white=TRUE, tidy=TRUE)
```
```{r get_scripts, include=F}
if (!require("RCurl")) install.packages("RCurl")
library ("RCurl")
dlScripts <- function (scriptNames) {
  fileHolder <- getURL(paste(githubRepo, scriptNames, sep = "/"), ssl.verifypeer = FALSE)
  fileConn<-file(scriptNames)
  writeLines(fileHolder, fileConn)
  close(fileConn)
  }
githubRepo <- "https://raw.githubusercontent.com/GallupGovt/ngs2/master/cycle2/analysis"
scriptNames <- c("effects.R", "wrangle.R", "analytics.R", "Active_Learning.R", "h1.1.R", "h1.2.R", 
                 "h1.3.R", "h2.1.R", "h2.2.R", "h2.3.R", "h3.1.R", "h3.2.R", "h3.3.R", "h3.4.R", "h3.5.R")
lapply(scriptNames, dlScripts)
```

```{r run_scripts, include=F}
# run scripts in order
source("run_scripts.R")
nIter<-500
source("analytics.R")
nIter<-10000
```

# Introduction

Social science has an incoherency problem arising from a historical focus on theory development, which has been recently augmented by a credibility crisis associated with failures in reproducibility, replicability and generalizability of most empirical research. The Gallup WITNESS team proposes advances in each of these areas through the implementation of next generation social science methods, including pre-registration of formalized predictions from multiple models and theories, multifactorial adaptive experiments in immersive environments with large, representative general population samples, probabilistic quantification of multiple sources of uncertainty and fully-transparent and automatically reproducible analytical pipelines. 

To demonstrate and validate these advances, we propose an experimental approach to explore the phenomenon of group motivation to innovate under competition and uncertainty. Evidence from the literature is incoherent regarding the extent to which groups are influenced by uncertainty in their decisions to pursue innovative alternatives, particularly in competitive environments. In this study, we will recruit up to 5,000 participants to participate in a multi-player online gaming platform where they are faced with a resource maximization challenge, which they can tackle using multiple tools and strategies. We will randomize participants according to 14 different variables, in a factorial space with 208 levels to test a total of 32 predictions and compare their explanatory power for group motivation to innovate.  We implement fully-Bayesian inference for hypothesis testing, an active learning pipeline for adaptive experimental design, and machine learning algorithms for data exploration. 

+ Pre-registration form is available at https://osf.io/6frkt/
+ Design and analytic details are available at https://osf.io/g8uv3/

# Hypotheses, variables and expected effect sizes

We present next an exhaustive list of confirmatory tests, including all hypotheses and associated predictions, along with the predicted effect sizes used in the priors. As an example, for prediction 1.1.1. variable X_1.1 can take four levels (no competition, low competition, balanced competition, high competition), of which the "no competition" level is the reference category, and the others are hypothesized to have an effect of log odds = -1.45, 1.45, and -1.45 respectively, to align with our prediction that perceived inter-group competition will have a large effect on group motivation to innovate, following an inverse u-shaped relationship. 

```{r hypotheses, echo=FALSE}
tests <- read.csv(url("https://raw.githubusercontent.com/GallupGovt/ngs2/master/cycle2/analysis/Tests.csv"),header = TRUE, sep = ',')
datatable(tests, 
          caption = "Experimental Hypotheses, Variables, and Expected Effect Sizes",
          options = list(
              scrollX = TRUE,
              scrollCollapse = TRUE))
```

# Data used for the prediction

* Valid experimental instances included games with at least three players and one tool choice. 
* Since October 25, 2018, Gallup has: 
    + Ran a total of `r nGames` valid instances.
    + Obtained usable data from a total of `r nPlayers` players.
    + Completed data collection for `r nConditions` of the 208 experimental conditions in the full factorial space.

`r barplot(table(dates$date.time))`

Variables used and a random sample of rows from the final analytical dataset are summarized below. 

```{r data header}
names(factorial)
datatable(sample_n(factorial, 5), 
          caption = "Randomly selected rows of processed data.",
          options = list(
              scrollX = TRUE,
              scrollCollapse = TRUE))
```

# Descriptives
```{r descriptives}
# Number of rounds
nByround=factorial%>%
  group_by(round)%>%
  summarise(counts  = n())
nChoices<-sum(nByround$counts)
nMax<- max(nByround$counts)
ggplot(data=nByround, aes(x=round, y=counts)) +
  geom_bar(stat="identity") +
  ggtitle("Number of Choices") + 
  xlab("Round") +
  ylab("Total Choices by Round")+
  annotate("text", x=7, y=nMax*1.15, label = paste("Total to date =", nChoices, "valid decisions in 13 rounds")) +
  scale_y_continuous(limits = c(0, nMax*1.25))
# By tool choice
factorial.tools<-subset(factorial, tools!="9" & tools!="11" & tools!="12")
factorial.tools$innovation2<- as.numeric(factorial.tools$innovation)-1
tool_rate1=factorial.tools%>%
  group_by(tools)%>%
  summarise(rate_inn=mean(innovation2, na.rm=TRUE))
ggplot(data=tool_rate1, aes(x=tools, y=rate_inn)) +
  geom_bar(stat="identity") +
  ggtitle("Innovative Choices by Tool Choice") + 
  xlab("Tool Choice") +
  ylab("Innovative Choice Rate")

# Check Test Items

factorial.tests<-subset(factorial, tools=="9" | tools=="11" | tools=="12")
factorial.tests$correct <- 0
factorial.tests$correct[factorial.tests$tools=="9" & factorial.tests$leaderChoice=="TNTbarrel"] <- 1
factorial.tests$correct[factorial.tests$tools=="11" & factorial.tests$leaderChoice=="SatchelCharge"] <- 1
factorial.tests$correct[factorial.tests$tools=="12" & factorial.tests$leaderChoice=="SatchelCharge"] <- 1
tool_rate2 <- factorial.tests %>%
  group_by(tools, correct) %>%
  summarise(counts  = n()) 
ggplot(tool_rate2, aes(x = tools, y = counts)) +
  geom_bar(
  aes(color = correct, fill = correct),
  stat = "identity", position = position_stack()) +
  theme(legend.position='none') +
  ggtitle("Correct Choices for Check Test Items") + 
  xlab("Check Test Item") +
  ylab("Number of Correct Choices")
tool_rate3 <- factorial.tests %>%
  group_by(matchid) %>%
  summarise(mean(correct)) 
allWrong<-as.data.frame(tool_rate3  %>% filter(`mean(correct)` == 0))
```

+ Check test items show that tool choices were adequately understood by participants. 
+ Only `r length(allWrong$matchid)` games failed all three check test items. 

```{r allwrong}
allWrong
```

# General effects

All hypothesis tests and effect size estimations are conducted within a Bayesian framework, using Bayesian Generalized Linear Mixed Models (GLMMs). Because repeated measures from the same game are not independent, all estimations will include a group random effect, and fixed effects for the corresponding independent variables. 

$ln(p/(1-p))_{ij} = \beta_{0} + \beta_{1}X_{j} + \beta_{2}Y_{ij} + \beta_{3} (X_{j}*Y_{ij}) +u_{j} + \epsilon_{ij}$

Where the log odds of the probability of innovation for each decision $i$ in each game $j$ are a function of a constant term $\beta_0$ (intercept); an experimentally manipulated independent 2-level factor $X$ that varies for each game $j$, with unknown coefficients $\beta_1$; an experimentally manipulated independent variable $Y$, that varies for each game $j$ and each measure $i$, with unknown coefficients $\beta_2$; a two-way interaction $(X_j*Y_ij)$ between both experimental variables, with unknown coefficients $\beta_3$; a group random effect $u_j$, and a residual error term $\epsilon_{ij}$. 

We present next the posterior distribution of the coefficients for the full-factorial model, using the equation above. 

```{r glmmoverall}
glmmoverall
posteriorAreas
```

# Hypothesis Testing

We estimate causal effects for all the predictions under each hypothesis using Bayesian applied regression modelling. We quantify the change from prior to posterior model odds based on observed data to compare competing predictions in terms of Bayes factors (see Alston et al., 2005, for a general discussion). 

Posterior predictive distributions and posterior parameter distributions are sampled using Hamiltonian MCMC (e.g. Hoffman & Gelman, 2014), with 3 Markov chains and 10,000 iterations. The posterior probability distributions for each prediction are summarized using the mean and the central 95% interval. Since we are primarily concerned with effect size estimation and model optimization within a Bayesian framework, correction for multiple comparisons do not apply (Gelman, Hill, & Yajima, 2012). 

As an example of hypothesis testing via Bayes Factors, we examine prediction 1.1.1: 

+ 1.1.1. Perceived inter-group competition will show an inverse u-shaped relationship with motivation to innovate: when competition is either too strong or too weak, motivation to innovate will decrease.

```{r h1.1, echo=F, include=F}
nIter<-5000
source("h1.1.R")
```
The posterior distribution of the coefficients is plotted below, including the three competition levels (Low, Medium, High). The "no competition" condition serves as the reference category. 

```{r h1.1.post, echo=T, include=T}
plotsh1.1.1<-bayesPlotter3 (glmm1.1.test, "h1.1.locomp", "h1.1.medcomp", "h1.1.hicomp", test.SD, "h1.11", "h1.12", "h1.13", testnull)
plotsh1.1.1[[1]]
```

We then estimate the likelihood that the observed data were produced by the hypothesized generating model (1.1.1. inverse u-shaped relationship), and compare that to the likelihood that the observed data were produced by a null model. The Bayes factor is the ratio of the marginal likelihoods of two models, where the marginal likelihood of a model is the probability of the data given a model and quantifies how well the model has predicted the observed data. We calculate the Bayes Factor from the ratio of the likelihood of prediction 1.1.1. to the alternative prediction. The resulting Bayes Factor (BF = `r testnullBF`) indicates the support for h1.1.1 over the null (BF = 1 indicates no preference, BF>10 indicates strong preference for h1.1.1, BF<0.1 indicates strong preference for h1.1.2). 

```{r h1.1.test, include=T}
plotsh1.1.1[[2]]
```

+ 1.1.2. Increased levels of perceived intergroup competition will decrease group motivation to innovate.

```{r h1.1.alt1, include=T}
plotIters<-nIter*1.5
draws <- as.data.frame(glmm1.1.alt1)
a <- rnorm(plotIters, mean=logodds[["h1.1.medcomp"]], sd=test.SD)
b <- rnorm(plotIters, mean=0, sd=test.SD)
c <- rnorm(plotIters, mean=logodds[["h1.1.locomp"]], sd=test.SD)
d <- draws[["h1.11"]]
e <- draws[["h1.12"]]
f <- draws[["h1.13"]]
plotdf <- data.frame(value=c(a, b, c, d, e, f), 
                     Distribution=c(rep("Prior", plotIters*3),
                                    rep("Posterior", plotIters*3)), 
                     Level=c(rep("h1.1.locomp", plotIters),
                             rep("h1.1.medcomp", plotIters),
                             rep("h1.1.hicomp", plotIters), 
                             rep("h1.1.locomp", plotIters),
                             rep("h1.1.medcomp", plotIters),
                             rep("h1.1.hicomp", plotIters)))
frame.posterior<-subset(plotdf, Distribution=="Posterior")
ggplot(plotdf, aes(value, fill=Level, linetype=Distribution)) + 
  geom_density(alpha=0.4) + 
  scale_x_continuous(limits = c(-5, 5)) + 
  scale_y_continuous(limits = c(0, 5)) +
  annotate("text", x=2, y=1.7, label = paste(" Alt1 vs Null BF = ", sprintf("%0.2f", alt1null))) +
  geom_vline(xintercept = 0, linetype="dashed")
```
+ 1.1.3. Increased levels of perceived intergroup competition will increase group motivation to innovate.

```{r h1.1.alt2, include=T}
draws <- as.data.frame(glmm1.1.alt2)
a <- rnorm(plotIters, mean=logodds[["h1.1.locomp"]], sd=test.SD)
b <- rnorm(plotIters, mean=0, sd=test.SD)
c <- rnorm(plotIters, mean=logodds[["h1.1.medcomp"]], sd=test.SD)
d <- draws[["h1.11"]]
e <- draws[["h1.12"]]
f <- draws[["h1.13"]]
plotdf <- data.frame(value=c(a, b, c, d, e, f), 
                     Distribution=c(rep("Prior", plotIters*3),
                                    rep("Posterior", plotIters*3)), 
                     Level=c(rep("h1.1.locomp", plotIters),
                             rep("h1.1.medcomp", plotIters),
                             rep("h1.1.hicomp", plotIters), 
                             rep("h1.1.locomp", plotIters),
                             rep("h1.1.medcomp", plotIters),
                             rep("h1.1.hicomp", plotIters)))
frame.posterior<-subset(plotdf, Distribution=="Posterior")
ggplot(plotdf, aes(value, fill=Level, linetype=Distribution)) + 
  geom_density(alpha=0.4) + 
  scale_x_continuous(limits = c(-5, 5)) + 
  scale_y_continuous(limits = c(0, 5)) +
  annotate("text", x=2, y=1.7, label = paste(" Alt2 vs Null BF = ", sprintf("%0.2f", alt2null))) +
  geom_vline(xintercept = 0, linetype="dashed")
```

The comparisons indicate overwhelming support for the null hypothesis of no effect of competition on motivation to innovate. 

# Summary of Hypothesis Tests

```{r BFs, include=T}
kable(BFs1.1, caption = "Bayes Factors - All Predictions")
```

# Active Learning

We finally deploy and Active Learning approach to minimize posterior uncertainty, where, given a dataset $(x_1,y_1),. ,(x_n,y_n )$, pick a new location $x \epsilon D$ to query the corresponding $x$ such that a large amount of information is gained according to some measure. We prioritize experimental condition based on  measures of variance and entropy. 

```{r active_learning, echo=FALSE}
source("Active_Learning.R")
```

Active learning will be particularly useful to optimize sample allocation to adequately power those hypotheses with the greatest information gains. 
